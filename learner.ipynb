{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP learning for bot (conservative twitter bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import seaborn as cb\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk \n",
    "from mosestokenizer import MosesDetokenizer \n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\djona\\AppData\\Local\\Temp\\ipykernel_9660\\2216374768.py:7: FutureWarning: content is deprecated, use rawContent instead\n",
      "  tweets_con.append([tweet.content]) # declare the attributes to be returned\n",
      "Tweet 1575223201269006336 contains an app icon medium key '4_1594606217359462401' on app 'iphone_app'/'1142951331', but the corresponding medium is missing; dropping\n",
      "Tweet 1575223201269006336 contains an app icon medium key '4_1594606217359462401' on app 'ipad_app'/'1142951331', but the corresponding medium is missing; dropping\n"
     ]
    }
   ],
   "source": [
    "tweets_con = []\n",
    "\n",
    "#scrape data and append tweets to list\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('#conservative').get_items()): # declare a username\n",
    "  if i>50000: #number of tweets you want to scrape\n",
    "    break\n",
    "  tweets_con.append([tweet.content]) # declare the attributes to be returned\n",
    "\n",
    "# creating a dataframe from the list\n",
    "tweets_con_df = pd.DataFrame(tweets_con, columns=['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating json of tweets for preservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"tweet_con.json\",'w')\n",
    "j = json.dumps(tweets_con, default = str)\n",
    "file.write(j)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    cleaned = re.sub(r\"http\\S+\",\"\", txt)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = [clean_text(elements) for elements in tweets_con_df.Text]\n",
    "tt = TweetTokenizer()\n",
    "normalize = [tt.tokenize(elements.lower()) for elements in cleaned_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\djona\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words = set(stopwords.words('english'))\n",
    "#stop_words.add('#conservative')\n",
    "\n",
    "# remove stop words and punctuation\n",
    "#complete = [[w for w in words if not w in stop_words if w.isalnum()] for words in normalize]\n",
    "complete = [[w for w in words if w.isalnum()] for words in normalize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngram splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = [list(ngrams(words,3)) for words in complete]\n",
    "bigram = [list(ngrams(words,2)) for words in complete]\n",
    "quadgram = [list(ngrams(words,4)) for words in complete]\n",
    "trigramC = list(itertools.chain(*trigram))\n",
    "bigramC = list(itertools.chain(*bigram))\n",
    "quadgramC = list(itertools.chain(*quadgram))\n",
    "tokenC = list(itertools.chain(*complete))\n",
    "token_counter = collections.Counter(tokenC)\n",
    "trigram_counter = collections.Counter(trigramC)\n",
    "bigram_counter = collections.Counter(bigramC)\n",
    "quadgram_counter = collections.Counter(quadgramC)\n",
    "trigram_total = sum(trigram_counter.values())\n",
    "bigram_total = sum(bigram_counter.values())\n",
    "quadgram_total = sum(quadgram_counter.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_rel_freq = {k: (v / bigram_total)+1 for (k,v) in bigram_counter.items()}\n",
    "with open(\"models/bigram_tweet.pkl\",\"wb\") as outfile:\n",
    "    pickle.dump(bigram_rel_freq, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_rel_freq = {k: (v / trigram_total)+1 for (k,v) in trigram_counter.items()}\n",
    "with open(\"models/trigram_tweet.pkl\",\"wb\") as outfile:\n",
    "    pickle.dump(trigram_rel_freq, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadgram_rel_freq = {k: (v / quadgram_total)+1 for (k,v) in quadgram_counter.items()}\n",
    "with open(\"models/quadgram_tweet.pkl\",\"wb\") as outfile:\n",
    "    pickle.dump(quadgram_rel_freq, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator based on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_gen(seed, num):\n",
    "    if(num == 2):\n",
    "        with open(\"models/bigram_tweet.pkl\", \"rb\") as infile:\n",
    "            bigrams = pickle.load(infile)\n",
    "        bgms = {k: v for (k,v) in bigrams.items() if k[:2] == seed}\n",
    "        wds = [e[2] for e in bgms.keys()]\n",
    "        if wds:\n",
    "            weights = [float(e) for e in bgms.values()]\n",
    "            return random.choices(population=wds,weights=weights)[0]\n",
    "        else:\n",
    "            w = random.choice(list(token_counter.keys()))\n",
    "            return w\n",
    "    if(num == 3):\n",
    "        with open(\"models/trigram_tweet.pkl\", \"rb\") as infile:\n",
    "            trigrams = pickle.load(infile)\n",
    "        tgms = {k: v for (k,v) in trigrams.items() if k[:2] == seed}\n",
    "        wds = [e[2] for e in tgms.keys()]\n",
    "        if wds:\n",
    "            weights = [float(e) for e in tgms.values()]\n",
    "            return random.choices(population=wds,weights=weights)[0]\n",
    "        else:\n",
    "            w = random.choice(list(token_counter.keys()))\n",
    "            return w\n",
    "    if(num == 4):\n",
    "        with open(\"models/quadgram_tweet.pkl\", \"rb\") as infile:\n",
    "            quadgrams = pickle.load(infile)\n",
    "        qgms = {k: v for (k,v) in quadgrams.items() if k[:2] == seed}\n",
    "        wds = [e[2] for e in qgms.keys()]\n",
    "        if wds:\n",
    "            weights = [float(e) for e in qgms.values()]\n",
    "            return random.choices(population=wds,weights=weights)[0]\n",
    "        else:\n",
    "            w = random.choice(list(token_counter.keys()))\n",
    "            return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = list(random.choice(list(trigram_rel_freq.keys())))\n",
    "for i in range(12):\n",
    "    w = ngram_gen((line[-2], line[-1]),3)\n",
    "    line.append(w)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2cb316dc1cb7649520000078faeeab63ab4993f92669687fc181249e534c3ab9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
